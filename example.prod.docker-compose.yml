version: "2"
services:
    db:
        image: gwul/sfm-ui-db:1.8.0
        environment:
            - POSTGRES_PASSWORD
            - TZ
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
        restart: always
    mq:
        image: gwul/sfm-rabbitmq:1.8.0
        hostname: mq
        ports:
            # Opens up the ports for RabbitMQ management
            - "${RABBITMQ_MANAGEMENT_PORT}:15672"
        environment:
            - RABBITMQ_DEFAULT_USER=${RABBITMQ_USER}
            - RABBITMQ_DEFAULT_PASS=${RABBITMQ_PASSWORD}
            - TZ
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
        restart: always
    # These containers will exit on startup. That's OK.
    data:
        image: gwul/sfm-data:1.8.0
        volumes:
             - ${DATA_VOLUME}
        environment:
            - TZ
            - SFM_UID
            - SFM_GID
    processingdata:
        image: debian:jessie
        command: /bin/true
        volumes:
             - ${PROCESSING_VOLUME}
        environment:
            - TZ
    ui:
        image: gwul/sfm-ui:1.8.0
        ports:
            - "${SFM_PORT}:8080"
        links:
            - db:db
            - mq:mq
        environment:
            - SFM_DEBUG=False
            - SFM_APSCHEDULER_LOG=INFO
            - SFM_UI_LOG=INFO
            # This adds a 5 minute schedule option to speed testing.
            - SFM_FIVE_MINUTE_SCHEDULE=False
            # This adds a 100 item export segment for testing.
            - SFM_HUNDRED_ITEM_SEGMENT=False
            - TZ
            - SFM_SITE_ADMIN_NAME
            - SFM_SITE_ADMIN_EMAIL
            - SFM_SITE_ADMIN_PASSWORD
            - SFM_EMAIL_USER
            - SFM_EMAIL_PASSWORD
            - SFM_SMTP_HOST
            - SFM_HOST=${SFM_HOSTNAME}:${SFM_PORT}
            - SFM_CONTACT_EMAIL
            - TWITTER_CONSUMER_KEY
            - TWITTER_CONSUMER_SECRET
            - WEIBO_API_KEY
            - WEIBO_API_SECRET
            - TUMBLR_CONSUMER_KEY
            - TUMBLR_CONSUMER_SECRET
            - RABBITMQ_USER
            - RABBITMQ_PASSWORD
            - RABBITMQ_MANAGEMENT_PORT
            - POSTGRES_PASSWORD
            # To have some test accounts created.
            - LOAD_FIXTURES=False
            - SFM_REQS=release
            - DATA_VOLUME_THRESHOLD
            - PROCESSING_VOLUME_THRESHOLD
            - SFM_UID
            - SFM_GID
            - SFM_INSTITUTION_NAME
            - SFM_INSTITUTION_LINK
            - SFM_MONITOR_QUEUE_HOUR_INTERVAL
            - SFM_SCAN_FREE_SPACE_HOUR_INTERVAL
            - SFM_WEIBO_SEARCH_OPTION
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
            - processingdata
        restart: always
    uiconsumer:
        image: gwul/sfm-ui-consumer:1.8.0
        links:
            - db:db
            - mq:mq
            - ui:ui
        environment:
            - SFM_DEBUG=False
            - SFM_APSCHEDULER_LOG=INFO
            - SFM_UI_LOG=INFO
            - TZ
            - SFM_SITE_ADMIN_NAME
            - SFM_SITE_ADMIN_EMAIL
            - SFM_SITE_ADMIN_PASSWORD
            - SFM_EMAIL_USER
            - SFM_EMAIL_PASSWORD
            - SFM_SMTP_HOST
            - SFM_HOST=${SFM_HOSTNAME}:${SFM_PORT}
            - RABBITMQ_USER
            - RABBITMQ_PASSWORD
            - POSTGRES_PASSWORD
            - SFM_REQS=release
            - SFM_UID
            - SFM_GID
        volumes_from:
            - data
            - processingdata
        restart: always
# Twitter
    twitterrestharvester:
        image: gwul/sfm-twitter-rest-harvester:1.8.0
        links:
            - mq:mq
        environment:
            - TZ
            - DEBUG=False
            - RABBITMQ_USER
            - RABBITMQ_PASSWORD
            - SFM_REQS=release
            - HARVEST_TRIES=${TWITTER_REST_HARVEST_TRIES}
            - SFM_UID
            - SFM_GID
            - PRIORITY_QUEUES=False
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
        restart: always
    twitterpriorityrestharvester:
        image: gwul/sfm-twitter-rest-harvester:1.8.0
        links:
            - mq:mq
        environment:
            - TZ
            - DEBUG=False
            - RABBITMQ_USER
            - RABBITMQ_PASSWORD
            - SFM_REQS=release
            - HARVEST_TRIES=${TWITTER_REST_HARVEST_TRIES}
            - SFM_UID
            - SFM_GID
            - PRIORITY_QUEUES=True
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
        restart: always
    twitterstreamharvester:
        image: gwul/sfm-twitter-stream-harvester:1.8.0
        links:
            - mq:mq
        environment:
            - TZ
            - DEBUG=False
            - RABBITMQ_USER
            - RABBITMQ_PASSWORD
            - SFM_REQS=release
            - HARVEST_TRIES=${TWITTER_STREAM_HARVEST_TRIES}
            - SFM_UID
            - SFM_GID
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
        restart: always
    twitterrestexporter:
        image: gwul/sfm-twitter-rest-exporter:1.8.0
        links:
            - mq:mq
            - ui:api
        environment:
            - TZ
            - DEBUG=False
            - RABBITMQ_USER
            - RABBITMQ_PASSWORD
            - SFM_REQS=release
            - SFM_UID
            - SFM_GID
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
        restart: always
    twitterstreamexporter:
        image: gwul/sfm-twitter-stream-exporter:1.8.0
        links:
            - mq:mq
            - ui:api
        environment:
            - TZ
            - DEBUG=False
            - RABBITMQ_USER
            - RABBITMQ_PASSWORD
            - SFM_REQS=release
            - SFM_UID
            - SFM_GID
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
        restart: always

# FLICKR
    flickrharvester:
        image: gwul/sfm-flickr-harvester:1.8.0
        links:
            - mq:mq
        environment:
            - TZ
            - DEBUG=False
            - RABBITMQ_USER
            - RABBITMQ_PASSWORD
            - SFM_REQS=release
            - HARVEST_TRIES=${FLICKR_HARVEST_TRIES}
            - SFM_UID
            - SFM_GID
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
        restart: always
    flickrexporter:
        image: gwul/sfm-flickr-exporter:1.8.0
        links:
            - mq:mq
            - ui:api
        environment:
            - DEBUG=False
            - TZ
            - RABBITMQ_USER
            - RABBITMQ_PASSWORD
            - SFM_REQS=release
            - SFM_UID
            - SFM_GID
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
        restart: always

# WEB
    heritrix:
        image:  gwul/sfm-heritrix:1.8.0
        ports:
            # Opens up the port for Heritrix admin console.
            - "${HERITRIX_ADMIN_PORT}:8443"
        environment:
            - HERITRIX_USER
            - HERITRIX_PASSWORD
            # Memory for heritrix
            - JAVA_OPTS=-Xmx512M
            - SFM_UID
            - SFM_GID
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
        restart: always
    webharvester:
        image: gwul/sfm-web-harvester:1.8.0
        links:
            - mq:mq
            - heritrix:heritrix
        environment:
            - DEBUG=False
            - HERITRIX_CONTACT_URL
            - RABBITMQ_USER
            - RABBITMQ_PASSWORD
            - HERITRIX_USER
            - HERITRIX_PASSWORD
            - SFM_REQS=release
            - SFM_UID
            - SFM_GID
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
        restart: always

# WEIBO
    weiboharvester:
        image: gwul/sfm-weibo-harvester:1.8.0
        links:
            - mq:mq
        environment:
            - TZ
            - DEBUG=False
            - RABBITMQ_USER
            - RABBITMQ_PASSWORD
            - SFM_REQS=release
            - HARVEST_TRIES=${WEIBO_HARVEST_TRIES}
            - SFM_UID
            - SFM_GID
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
        restart: always
    weiboexporter:
        image: gwul/sfm-weibo-exporter:1.8.0
        links:
            - mq:mq
            - ui:api
        environment:
            - TZ
            - DEBUG=False
            - RABBITMQ_USER
            - RABBITMQ_PASSWORD
            - SFM_REQS=release
            - SFM_UID
            - SFM_GID
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
        restart: always

# TUMBLR
    tumblrharvester:
        image: gwul/sfm-tumblr-harvester:1.8.0
        links:
            - mq:mq
        environment:
            - TZ
            - DEBUG=False
            - RABBITMQ_USER
            - RABBITMQ_PASSWORD
            - SFM_REQS=release
            - HARVEST_TRIES=${TUMBLR_HARVEST_TRIES}
            - SFM_UID
            - SFM_GID
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
        restart: always
    tumblrexporter:
        image: gwul/sfm-tumblr-exporter:1.8.0
        links:
            - mq:mq
            - ui:api
        environment:
            - TZ
            - DEBUG=False
            - RABBITMQ_USER
            - RABBITMQ_PASSWORD
            - SFM_REQS=release
            - SFM_UID
            - SFM_GID
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
        restart: always

# PROCESSING
    # This container will exit on startup. That's OK.
    processing:
        image: gwul/sfm-processing:1.8.0
        links:
            - ui:api
        environment:
            - TZ
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data:ro
            - processingdata

# ELK
    # Multiple instances of this container can be included by duplicating this definition
    # and changing ports and command.
#    elasticsearch1:
#        image: gwul/sfm-elasticsearch:1.8.0
#        ports:
#          - "9200:9200"
#          - "9300:9300"
#        logging:
#            driver: json-file
#            options:
#                max-size: ${DOCKER_LOG_MAX_SIZE}
#                max-file: ${DOCKER_LOG_MAX_FILE}
#        # This setting supports the bootstrap_memory_lock=true, see
#        # https://www.elastic.co/guide/en/elasticsearch/reference/5.x/docker.html#_notes_for_production_use_and_defaults
#        ulimits:
#            memlock:
#              soft: -1
#              hard: -1
#        # To limit the total memory that elasticsearch instance can use if necessary
#        # mem_limit: 8g
#        cap_add:
#          - IPC_LOCK
#        volumes_from:
#          - data
#        environment:
#            # The cluster name and node name
#            #  see https://www.elastic.co/guide/en/elasticsearch/reference/5.x/important-settings.html#cluster.name
#            #  see https://www.elastic.co/guide/en/elasticsearch/reference/5.x/_basic_concepts.html
#            # To separates the nodes data, usually set different value for different instance
#            - cluster.name=sfm_prod_1
#            # set the unique node name
#            - node.name=sfm_es_node_1
#            # The amount of heap size for Elasticsearch.
#            # Set the minimum heap size (Xms) and maximum heap size (Xmx) to be equal to each other.
#            # For large indexes data, consider adding more heap size.
#            #  See https://www.elastic.co/guide/en/elasticsearch/reference/current/heap-size.html
#            - ES_JAVA_OPTS=-Xms4g -Xmx4g
#            # Ensure bootstrap.memory_lock is set to true as explained in "Disable swapping".
#            # For details: see
#            # https://www.elastic.co/guide/en/elasticsearch/reference/5.x/docker.html#_notes_for_production_use_and_defaults
#            - bootstrap.memory_lock=true
#            # Whether to turn on the monitoring for ElasticSearch. Setting for ElasticSearch and Kibana should be the same.
#            # See https://www.elastic.co/guide/en/x-pack/5.0/monitoring-settings.html
#            - xpack.monitoring.enabled=false
#            - TZ
#            - DEBUG=false
#            # setting waiting seconds for check apps dependencies
#            - WAIT_SECS=900
#        # Set the hostname uniquely for each elasticsearch instance container
#        hostname: sfm_es_1
#        restart: always
#
#    kibana1:
#        image: gwul/sfm-kibana:1.8.0
#        links:
#            - elasticsearch1:elasticsearch
#        ports:
#           - "5601:5601"
#        logging:
#            driver: json-file
#            options:
#                max-size: ${DOCKER_LOG_MAX_SIZE}
#                max-file: ${DOCKER_LOG_MAX_FILE}
#        volumes_from:
#          - data
#        environment:
#            # Suppress all logging output other than error messages.
#            # https://www.elastic.co/guide/en/kibana/5.x/_configuring_kibana_on_docker.html
#            - LOGGING_QUIET=true
#            # Setting the default app for kibana to load, Twitter or Weibo
#            - KIBANA_DEFAULTAPPID="dashboard/Twitter"
#            # Whether to turn on the monitoring for ElasticSearch. Setting for ElasticSearch and Kibana should be the same.
#            # See https://www.elastic.co/guide/en/x-pack/5.0/monitoring-settings.html
#            - XPACK_MONITORING_ENABLED=false
#            - TZ
#            - DEBUG=false
#            # setting waiting seconds for check apps dependencies
#            - WAIT_SECS=900
#        # Set the hostname uniquely for each elk container
#        hostname: sfm_kibana_1
#        restart: always
#
#    logstash1:
#        image: gwul/sfm-logstash:1.8.0
#        links:
#            - mq:mq
#            - elasticsearch1:elasticsearch
#            - kibana1:kibana
#        ports:
#          - "5000:5000"
#        volumes_from:
#          - data
#        hostname: sfm_logstash_1
#        # To limit ELK loading to a specific collection set.
#        # command: --collection-set=ce112ac915254ea0a2899a70e668a460
#        environment:
#            - LS_JAVA_OPTS=-Xms2g -Xmx2g
#            - TZ
#            - DEBUG=false
#            - SFM_REQS=release
#            - RABBITMQ_USER
#            - RABBITMQ_PASSWORD
#            - SFM_UID
#            - SFM_GID
#            # setting waiting seconds for check apps dependencies
#            - WAIT_SECS=900
#            - SFM_UPGRADE_REQS=${UPGRADE_REQS}
#        restart: always
#
#    #  Example of the second instance and default dashboard for Weibo
#    elasticsearch2:
#        image: gwul/sfm-elasticsearch:1.8.0
#        ports:
#          - "9201:9200"
#          - "9301:9300"
#        logging:
#            driver: json-file
#            options:
#                max-size: ${DOCKER_LOG_MAX_SIZE}
#                max-file: ${DOCKER_LOG_MAX_FILE}
#        # This setting supports the bootstrap_memory_lock=true, see
#        # https://www.elastic.co/guide/en/elasticsearch/reference/5.x/docker.html#_notes_for_production_use_and_defaults
#        ulimits:
#            memlock:
#              soft: -1
#              hard: -1
#        # To limit the total memory that elasticsearch instance can use if necessary
#        # mem_limit: 8g
#        cap_add:
#          - IPC_LOCK
#        volumes_from:
#          - data
#        environment:
#            # The cluster name and node name
#            #  see https://www.elastic.co/guide/en/elasticsearch/reference/5.x/important-settings.html#cluster.name
#            #  see https://www.elastic.co/guide/en/elasticsearch/reference/5.x/_basic_concepts.html
#            # To separates the nodes data, usually set different value for different instance
#            - cluster.name=sfm_prod_2
#            # set the unique node name
#            - node.name=sfm_es_node_2
#            # The amount of heap size for Elasticsearch.
#            # Set the minimum heap size (Xms) and maximum heap size (Xmx) to be equal to each other.
#            # For large indexes data, consider adding more heap size.
#            #  See https://www.elastic.co/guide/en/elasticsearch/reference/current/heap-size.html
#            - ES_JAVA_OPTS=-Xms4g -Xmx4g
#            # Ensure bootstrap.memory_lock is set to true as explained in "Disable swapping".
#            # For details: see
#            # https://www.elastic.co/guide/en/elasticsearch/reference/5.x/docker.html#_notes_for_production_use_and_defaults
#            - bootstrap.memory_lock=true
#            # Whether to turn on the monitoring for ElasticSearch. Setting for ElasticSearch and Kibana should be the same.
#            # See https://www.elastic.co/guide/en/x-pack/5.0/monitoring-settings.html
#            - xpack.monitoring.enabled=false
#            - TZ
#            - DEBUG=false
#            # setting waiting seconds for check apps dependencies
#            - WAIT_SECS=900
#        # Set the hostname uniquely for each elasticsearch instance container
#        hostname: sfm_es_2
#        restart: always
#
#    kibana2:
#        image: gwul/sfm-kibana:1.8.0
#        links:
#            - elasticsearch2:elasticsearch
#        ports:
#           - "5602:5601"
#        logging:
#            driver: json-file
#            options:
#                max-size: ${DOCKER_LOG_MAX_SIZE}
#                max-file: ${DOCKER_LOG_MAX_FILE}
#        volumes_from:
#          - data
#        environment:
#            # Suppress all logging output other than error messages.
#            - LOGGING_QUIET=true
#            # Setting the default app for kibana to load, Twitter or Weibo
#            - KIBANA_DEFAULTAPPID="dashboard/Weibo"
#            # Whether to turn on the monitoring for ElasticSearch. Setting for ElasticSearch and Kibana should be the same.
#            # See https://www.elastic.co/guide/en/x-pack/5.0/monitoring-settings.html
#            - XPACK_MONITORING_ENABLED=false
#            - TZ
#            - DEBUG=false
#            # setting waiting seconds for check apps dependencies
#            - WAIT_SECS=900
#        # Set the hostname uniquely for each kibana instance
#        hostname: sfm_kibana_2
#        restart: always
#
#    logstash2:
#        image: gwul/sfm-logstash:1.8.0
#        links:
#            - mq:mq
#            - elasticsearch2:elasticsearch
#            - kibana2:kibana
#        ports:
#          - "5001:5000"
#        volumes_from:
#          - data
#        hostname: sfm_logstash_2
#        # To limit ELK loading to a specific collection set.
#        # command: --collection-set=1f0066fd9c464624aed612ddbf1fb407
#        environment:
#            - LS_JAVA_OPTS=-Xms2g -Xmx2g
#            - TZ
#            - DEBUG=false
#            - SFM_REQS=release
#            - RABBITMQ_USER
#            - RABBITMQ_PASSWORD
#            - SFM_UID
#            - SFM_GID
#            # setting waiting seconds for check apps dependencies
#            - WAIT_SECS=900
#            - SFM_UPGRADE_REQS=${UPGRADE_REQS}
#        restart: always

