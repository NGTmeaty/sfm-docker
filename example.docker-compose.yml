version: "2"
services:
    db:
        image: gwul/sfm-ui-db:master
        environment:
            - POSTGRES_PASSWORD
            - TZ
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
        restart: always
    mq:
        image: gwul/sfm-rabbitmq:master
        hostname: mq
        ports:
            # Opens up the ports for RabbitMQ management
            - "${RABBITMQ_MANAGEMENT_PORT}:15672"
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        environment:
            - RABBITMQ_DEFAULT_USER=${RABBITMQ_USER}
            - RABBITMQ_DEFAULT_PASS=${RABBITMQ_PASSWORD}
            - TZ
        volumes_from:
            - data
        restart: always
    # These containers will exit on startup. That's OK.
    data:
        image: gwul/sfm-data:master
        volumes:
             - ${DATA_VOLUME}
        environment:
            - TZ
            - SFM_UID
            - SFM_GID
    processingdata:
        image: debian:jessie
        command: /bin/true
        volumes:
             - ${PROCESSING_VOLUME}
        environment:
            - TZ
    ui:
        image: gwul/sfm-ui:${UI_TAG}
        ports:
            - "${SFM_PORT}:8080"
        links:
            - db:db
            - mq:mq
        environment:
            - SFM_DEBUG=${DEBUG}
            - SFM_APSCHEDULER_LOG=DEBUG
            - SFM_UI_LOG=DEBUG
            # This adds a 5 minute schedule option to speed testing.
            - SFM_FIVE_MINUTE_SCHEDULE=${FIVE_MINUTE_SCHEDULE}
            # This adds a 100 item export segment for testing.
            - SFM_HUNDRED_ITEM_SEGMENT=${HUNDRED_ITEM_SEGMENT}
            - TZ
            - SFM_SITE_ADMIN_NAME
            - SFM_SITE_ADMIN_EMAIL
            - SFM_SITE_ADMIN_PASSWORD
            - SFM_EMAIL_USER
            - SFM_EMAIL_PASSWORD
            - SFM_SMTP_HOST
            - SFM_HOST=${SFM_HOSTNAME}:${SFM_PORT}
            - SFM_HOSTNAME
            - SFM_CONTACT_EMAIL
            - TWITTER_CONSUMER_KEY
            - TWITTER_CONSUMER_SECRET
            - WEIBO_API_KEY
            - WEIBO_API_SECRET
            - TUMBLR_CONSUMER_KEY
            - TUMBLR_CONSUMER_SECRET
            - RABBITMQ_USER
            - RABBITMQ_PASSWORD
            - RABBITMQ_MANAGEMENT_PORT
            - POSTGRES_PASSWORD
            # To have some test accounts created.
            - LOAD_FIXTURES=${LOAD_FIXTURES}
            - SFM_REQS=${UI_REQS}
            - DATA_VOLUME_THRESHOLD
            - PROCESSING_VOLUME_THRESHOLD
            - SFM_UID
            - SFM_GID
            - SFM_UPGRADE_REQS=${UPGRADE_REQS}
            - SFM_INSTITUTION_NAME
            - SFM_INSTITUTION_LINK
            - SFM_MONITOR_QUEUE_HOUR_INTERVAL
            - SFM_SCAN_FREE_SPACE_HOUR_INTERVAL
            - SFM_WEIBO_SEARCH_OPTION
            - SFM_USE_HTTPS
            # For ngninx-proxy
            - VIRTUAL_HOST=${SFM_HOSTNAME}
            - VIRTUAL_PORT=${SFM_PORT}
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
            - processingdata
#        volumes:
#            - "../sfm-ui:/opt/sfm-ui"
#            # To also link in a local sfm-utils, uncomment this and set UI_REQS to "dev" in .env
#            - "../sfm-utils:/opt/sfm-utils"
#    # For running SFM with HTTPS
#    # When using this, in .env, SFM_PORT must be 8080 and USE_HTTPS must be True.
#    # For more information on configuration of nginx-proxy, see https://github.com/jwilder/nginx-proxy
#    nginx-proxy:
#        image: jwilder/nginx-proxy
#        ports:
#            - "443:443"
#            - "80:80"
#        environment:
#            - DEFAULT_HOST=${SFM_HOSTNAME}
#        logging:
#            driver: json-file
#            options:
#                max-size: ${DOCKER_LOG_MAX_SIZE}
#                max-file: ${DOCKER_LOG_MAX_FILE}
#        volumes:
#            - /var/run/docker.sock:/tmp/docker.sock:ro
#            # This should point to your local key and certificate
#            # Make sure in the cert that the server cert comes before the intermediate certs.
#            - "./server.crt:/etc/nginx/certs/${SFM_HOSTNAME}.crt"
#            - "./server.key:/etc/nginx/certs/${SFM_HOSTNAME}.key"
    uiconsumer:
        image: gwul/sfm-ui-consumer:master
        links:
            - db:db
            - mq:mq
            - ui:ui
        environment:
            - SFM_DEBUG=${DEBUG}
            - SFM_APSCHEDULER_LOG=DEBUG
            - SFM_UI_LOG=DEBUG
            - TZ
            - SFM_SITE_ADMIN_NAME
            - SFM_SITE_ADMIN_EMAIL
            - SFM_SITE_ADMIN_PASSWORD
            - SFM_EMAIL_USER
            - SFM_EMAIL_PASSWORD
            - SFM_SMTP_HOST
            - SFM_HOST=${SFM_HOSTNAME}:${SFM_PORT}
            - RABBITMQ_USER
            - RABBITMQ_PASSWORD
            - POSTGRES_PASSWORD
            - SFM_REQS=${UI_REQS}
            - SFM_UID
            - SFM_GID
            - SFM_UPGRADE_REQS=${UPGRADE_REQS}
            - SFM_USE_HTTPS
        volumes_from:
            - data
            - processingdata
#        volumes:
#            - "../sfm-ui:/opt/sfm-ui"
#            # To also link in a local sfm-utils, uncomment this and set UI_REQS to "dev" in .env
#            - "../sfm-utils:/opt/sfm-utils"
# Twitter
    twitterrestharvester:
        image: gwul/sfm-twitter-rest-harvester:master
        links:
            - mq:mq
        environment:
            - TZ
            - DEBUG
            - DEBUG_WARCPROX
            - RABBITMQ_USER
            - RABBITMQ_PASSWORD
            - SFM_REQS=${TWITTER_REQS}
            - HARVEST_TRIES=${TWITTER_REST_HARVEST_TRIES}
            - SFM_UID
            - SFM_GID
            - SFM_UPGRADE_REQS=${UPGRADE_REQS}
            - PRIORITY_QUEUES=False
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
#        volumes:
#            - "../sfm-twitter-harvester:/opt/sfm-twitter-harvester"
#            - "../sfm-utils:/opt/sfm-utils"
#            - "../warcprox:/opt/warcprox"
    twitterpriorityrestharvester:
        image: gwul/sfm-twitter-rest-harvester:master
        links:
            - mq:mq
        environment:
            - TZ
            - DEBUG
            - DEBUG_WARCPROX
            - RABBITMQ_USER
            - RABBITMQ_PASSWORD
            - SFM_REQS=${TWITTER_REQS}
            - HARVEST_TRIES=${TWITTER_REST_HARVEST_TRIES}
            - SFM_UID
            - SFM_GID
            - SFM_UPGRADE_REQS=${UPGRADE_REQS}
            - PRIORITY_QUEUES=True
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
#        volumes:
#            - "../sfm-twitter-harvester:/opt/sfm-twitter-harvester"
#            - "../sfm-utils:/opt/sfm-utils"
#            - "../warcprox:/opt/warcprox"
    twitterstreamharvester:
        image: gwul/sfm-twitter-stream-harvester:master
        links:
            - mq:mq
        environment:
            - TZ
            - DEBUG
            - DEBUG_WARCPROX
            - RABBITMQ_USER
            - RABBITMQ_PASSWORD
            - SFM_REQS=${TWITTER_REQS}
            - HARVEST_TRIES=${TWITTER_STREAM_HARVEST_TRIES}
            - SFM_UID
            - SFM_GID
            - SFM_UPGRADE_REQS=${UPGRADE_REQS}
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
#        volumes:
#            - "../sfm-twitter-harvester:/opt/sfm-twitter-harvester"
#            - "../sfm-utils:/opt/sfm-utils"
#            - "../warcprox:/opt/warcprox"
    twitterrestexporter:
        image: gwul/sfm-twitter-rest-exporter:master
        links:
            - mq:mq
            - ui:api
        environment:
            - TZ
            - DEBUG
            - RABBITMQ_USER
            - RABBITMQ_PASSWORD
            - SFM_REQS=${TWITTER_REQS}
            - SFM_UID
            - SFM_GID
            - SFM_UPGRADE_REQS=${UPGRADE_REQS}
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
#        volumes:
#            - "../sfm-twitter-harvester:/opt/sfm-twitter-harvester"
#            - "../sfm-utils:/opt/sfm-utils"
#            - "../warcprox:/opt/warcprox"
    twitterstreamexporter:
        image: gwul/sfm-twitter-stream-exporter:master
        links:
            - mq:mq
            - ui:api
        environment:
            - TZ
            - DEBUG
            - RABBITMQ_USER
            - RABBITMQ_PASSWORD
            - SFM_REQS=${TWITTER_REQS}
            - SFM_UID
            - SFM_GID
            - SFM_UPGRADE_REQS=${UPGRADE_REQS}
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
#        volumes:
#            - "../sfm-twitter-harvester:/opt/sfm-twitter-harvester"
#            - "../sfm-utils:/opt/sfm-utils"
#            - "../warcprox:/opt/warcprox"

# FLICKR
    flickrharvester:
        image: gwul/sfm-flickr-harvester:master
        links:
            - mq:mq
        environment:
            - TZ
            - DEBUG
            - DEBUG_WARCPROX
            - RABBITMQ_USER
            - RABBITMQ_PASSWORD
            - SFM_REQS=${FLICKR_REQS}
            - HARVEST_TRIES=${FLICKR_HARVEST_TRIES}
            - SFM_UID
            - SFM_GID
            - SFM_UPGRADE_REQS=${UPGRADE_REQS}
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
#        volumes:
#            - "../sfm-flickr-harvester:/opt/sfm-flickr-harvester"
#            - "../sfm-utils:/opt/sfm-utils"
#            - "../warcprox:/opt/warcprox"
    flickrexporter:
        image: gwul/sfm-flickr-exporter:master
        links:
            - mq:mq
            - ui:api
        environment:
            - DEBUG
            - TZ
            - RABBITMQ_USER
            - RABBITMQ_PASSWORD
            - SFM_REQS=${FLICKR_REQS}
            - SFM_UID
            - SFM_GID
            - SFM_UPGRADE_REQS=${UPGRADE_REQS}
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
#        volumes:
#            - "../sfm-flickr-harvester:/opt/sfm-flickr-harvester"
#            - "../sfm-utils:/opt/sfm-utils"
#            - "../warcprox:/opt/warcprox"
# WEB
    heritrix:
        image:  gwul/sfm-heritrix:master
        ports:
            # Opens up the port for Heritrix admin console.
            - "${HERITRIX_ADMIN_PORT}:8443"
        environment:
            - HERITRIX_USER
            - HERITRIX_PASSWORD
            # Memory for heritrix
            - JAVA_OPTS=-Xmx512M
            - SFM_UID
            - SFM_GID
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
    webharvester:
        image: gwul/sfm-web-harvester:master
        links:
            - mq:mq
            - heritrix:heritrix
        environment:
            - DEBUG
            - HERITRIX_CONTACT_URL
            - RABBITMQ_USER
            - RABBITMQ_PASSWORD
            - HERITRIX_USER
            - HERITRIX_PASSWORD
            - SFM_REQS=${WEB_REQS}
            - SFM_UID
            - SFM_GID
            - SFM_UPGRADE_REQS=${UPGRADE_REQS}
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
#        volumes:
#            - "../sfm-web-harvester:/opt/sfm-web-harvester"
#            - "../sfm-utils:/opt/sfm-utils"

# WEIBO
    weiboharvester:
        image: gwul/sfm-weibo-harvester:master
        links:
            - mq:mq
        environment:
            - TZ
            - DEBUG
            - DEBUG_WARCPROX
            - RABBITMQ_USER
            - RABBITMQ_PASSWORD
            - SFM_REQS=${WEIBO_REQS}
            - HARVEST_TRIES=${WEIBO_HARVEST_TRIES}
            - SFM_UID
            - SFM_GID
            - SFM_UPGRADE_REQS=${UPGRADE_REQS}
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
#        volumes:
#            - "../sfm-weibo-harvester:/opt/sfm-weibo-harvester"
#            - "../sfm-utils:/opt/sfm-utils"
#            - "../warcprox:/opt/warcprox"

    weiboexporter:
        image: gwul/sfm-weibo-exporter:master
        links:
            - mq:mq
            - ui:api
        environment:
            - TZ
            - DEBUG
            - RABBITMQ_USER
            - RABBITMQ_PASSWORD
            - SFM_REQS=${WEIBO_REQS}
            - SFM_UID
            - SFM_GID
            - SFM_UPGRADE_REQS=${UPGRADE_REQS}
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
#        volumes:
#            - "../sfm-weibo-harvester:/opt/sfm-weibo-harvester"
#            - "../sfm-utils:/opt/sfm-utils"
#            - "../warcprox:/opt/warcprox"

# TUMBLR
    tumblrharvester:
        image: gwul/sfm-tumblr-harvester:master
        links:
            - mq:mq
        environment:
            - TZ
            - DEBUG
            - DEBUG_WARCPROX
            - RABBITMQ_USER
            - RABBITMQ_PASSWORD
            - SFM_REQS=${TUMBLR_REQS}
            - HARVEST_TRIES=${TUMBLR_HARVEST_TRIES}
            - SFM_UID
            - SFM_GID
            - SFM_UPGRADE_REQS=${UPGRADE_REQS}
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
#        volumes:
#            - "../sfm-tumblr-harvester:/opt/sfm-tumblr-harvester"
#            - "../sfm-utils:/opt/sfm-utils"
#            - "../warcprox:/opt/warcprox"
    tumblrexporter:
        image: gwul/sfm-tumblr-exporter:master
        links:
            - mq:mq
            - ui:api
        environment:
            - TZ
            - DEBUG
            - RABBITMQ_USER
            - RABBITMQ_PASSWORD
            - SFM_REQS=${TUMBLR_REQS}
            - SFM_UID
            - SFM_GID
            - SFM_UPGRADE_REQS=${UPGRADE_REQS}
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data
#        volumes:
#            - "../sfm-tumblr-harvester:/opt/sfm-tumblr-harvester"
#            - "../sfm-utils:/opt/sfm-utils"
#            - "../warcprox:/opt/warcprox"

# PROCESSING
    # This container will exit on startup. That's OK.
    processing:
        image: gwul/sfm-processing:master
        links:
            - ui:api
        environment:
            - TZ
        logging:
            driver: json-file
            options:
                max-size: ${DOCKER_LOG_MAX_SIZE}
                max-file: ${DOCKER_LOG_MAX_FILE}
        volumes_from:
            - data:ro
            - processingdata

# ELK
    # Multiple instances of this container can be included by duplicating this definition
    # and changing ports and command.
#    elasticsearch1:
#        image: gwul/sfm-elasticsearch:master
#        ports:
#          - "9200:9200"
#          - "9300:9300"
#        logging:
#            driver: json-file
#            options:
#                max-size: ${DOCKER_LOG_MAX_SIZE}
#                max-file: ${DOCKER_LOG_MAX_FILE}
#        # This setting supports the bootstrap_memory_lock=true, see
#        # https://www.elastic.co/guide/en/elasticsearch/reference/5.x/docker.html#_notes_for_production_use_and_defaults
#        ulimits:
#            memlock:
#              soft: -1
#              hard: -1
#        # To limit the total memory that elasticsearch instance can use if necessary
#        # mem_limit: 8g
#        cap_add:
#          - IPC_LOCK
#        volumes_from:
#          - data
#        environment:
#            # The cluster name and node name
#            #  see https://www.elastic.co/guide/en/elasticsearch/reference/5.x/important-settings.html#cluster.name
#            #  see https://www.elastic.co/guide/en/elasticsearch/reference/5.x/_basic_concepts.html
#            # To separates the nodes data, usually set different value for different instance
#            - cluster.name=sfm_prod_1
#            # set the unique node name
#            - node.name=sfm_es_node_1
#            # The amount of heap size for Elasticsearch.
#            # Set the minimum heap size (Xms) and maximum heap size (Xmx) to be equal to each other.
#            # For large indexes data, consider adding more heap size.
#            #  See https://www.elastic.co/guide/en/elasticsearch/reference/current/heap-size.html
#            - ES_JAVA_OPTS=-Xms4g -Xmx4g
#            # Ensure bootstrap.memory_lock is set to true as explained in "Disable swapping".
#            # For details: see
#            # https://www.elastic.co/guide/en/elasticsearch/reference/5.x/docker.html#_notes_for_production_use_and_defaults
#            - bootstrap.memory_lock=true
#            # Whether to turn on the monitoring for ElasticSearch. Setting for ElasticSearch and Kibana should be the same.
#            # See https://www.elastic.co/guide/en/x-pack/5.0/monitoring-settings.html
#            - xpack.monitoring.enabled=false
#            - TZ
#            - DEBUG
#            # setting waiting seconds for check apps dependencies
#            - WAIT_SECS=900
#        # Set the hostname uniquely for each elasticsearch instance container
#        hostname: sfm_es_1
##        volumes:
##            - "../sfm-elk:/opt/sfm-elk"
#
#    kibana1:
#        image: gwul/sfm-kibana:master
#        links:
#            - elasticsearch1:elasticsearch
#        ports:
#           - "5601:5601"
#        logging:
#            driver: json-file
#            options:
#                max-size: ${DOCKER_LOG_MAX_SIZE}
#                max-file: ${DOCKER_LOG_MAX_FILE}
#        volumes_from:
#          - data
#        environment:
#            # Suppress all logging output other than error messages.
#            # https://www.elastic.co/guide/en/kibana/5.x/_configuring_kibana_on_docker.html
#            - LOGGING_QUIET=true
#            # Setting the default app for kibana to load, Twitter or Weibo
#            - KIBANA_DEFAULTAPPID="dashboard/Twitter"
#            # Whether to turn on the monitoring for ElasticSearch. Setting for ElasticSearch and Kibana should be the same.
#            # See https://www.elastic.co/guide/en/x-pack/5.0/monitoring-settings.html
#            - XPACK_MONITORING_ENABLED=false
#            - TZ
#            - DEBUG
#            # setting waiting seconds for check apps dependencies
#            - WAIT_SECS=900
#        # Set the hostname uniquely for each elk container
#        hostname: sfm_kibana_1
##        volumes:
##            - "../sfm-elk:/opt/sfm-elk"
#
#    logstash1:
#        image: gwul/sfm-logstash:master
#        links:
#            - mq:mq
#            - elasticsearch1:elasticsearch
#            - kibana1:kibana
#        ports:
#          - "5000:5000"
#        volumes_from:
#          - data
#        hostname: sfm_logstash_1
#        # To limit ELK loading to a specific collection set.
#        # command: --collection-set=ce112ac915254ea0a2899a70e668a460
#        environment:
#            - LS_JAVA_OPTS=-Xms2g -Xmx2g
#            - TZ
#            - DEBUG
#            - SFM_REQS=${ELK_REQS}
#            - RABBITMQ_USER
#            - RABBITMQ_PASSWORD
#            - SFM_UID
#            - SFM_GID
#            # setting waiting seconds for check apps dependencies
#            - WAIT_SECS=900
#            - SFM_UPGRADE_REQS=${UPGRADE_REQS}
#        volumes:
#            - "../sfm-elk:/opt/sfm-elk"
#            - "../sfm-twitter-harvester:/opt/sfm-twitter-harvester"
#            - "../sfm-weibo-harvester:/opt/sfm-weibo-harvester"
#            - "../sfm-utils:/opt/sfm-utils"
#
#    # Example of the second instance and default dashboard for Weibo
#    elasticsearch2:
#        image: gwul/sfm-elasticsearch:master
#        ports:
#          - "9201:9200"
#          - "9301:9300"
#        logging:
#            driver: json-file
#            options:
#                max-size: ${DOCKER_LOG_MAX_SIZE}
#                max-file: ${DOCKER_LOG_MAX_FILE}
#        # This setting supports the bootstrap_memory_lock=true, see
#        # https://www.elastic.co/guide/en/elasticsearch/reference/5.x/docker.html#_notes_for_production_use_and_defaults
#        ulimits:
#            memlock:
#              soft: -1
#              hard: -1
#        # To limit the total memory that elasticsearch instance can use if necessary
#        # mem_limit: 8g
#        cap_add:
#          - IPC_LOCK
#        volumes_from:
#          - data
#        environment:
#            # The cluster name and node name
#            #  see https://www.elastic.co/guide/en/elasticsearch/reference/5.x/important-settings.html#cluster.name
#            #  see https://www.elastic.co/guide/en/elasticsearch/reference/5.x/_basic_concepts.html
#            # To separates the nodes data, usually set different value for different instance
#            - cluster.name=sfm_prod_2
#            # set the unique node name
#            - node.name=sfm_es_node_2
#            # The amount of heap size for Elasticsearch.
#            # Set the minimum heap size (Xms) and maximum heap size (Xmx) to be equal to each other.
#            # For large indexes data, consider adding more heap size.
#            #  See https://www.elastic.co/guide/en/elasticsearch/reference/current/heap-size.html
#            - ES_JAVA_OPTS=-Xms4g -Xmx4g
#            # Ensure bootstrap.memory_lock is set to true as explained in "Disable swapping".
#            # For details: see
#            # https://www.elastic.co/guide/en/elasticsearch/reference/5.x/docker.html#_notes_for_production_use_and_defaults
#            - bootstrap.memory_lock=true
#            # Whether to turn on the monitoring for ElasticSearch. Setting for ElasticSearch and Kibana should be the same.
#            # See https://www.elastic.co/guide/en/x-pack/5.0/monitoring-settings.html
#            - xpack.monitoring.enabled=false
#            - TZ
#            - DEBUG
#            # setting waiting seconds for check apps dependencies
#            - WAIT_SECS=900
#        # Set the hostname uniquely for each elasticsearch instance container
#        hostname: sfm_es_2
##        volumes:
##            - "../sfm-elk:/opt/sfm-elk"
#
#    kibana2:
#        image: gwul/sfm-kibana:master
#        links:
#            - elasticsearch2:elasticsearch
#        ports:
#           - "5602:5601"
#        logging:
#            driver: json-file
#            options:
#                max-size: ${DOCKER_LOG_MAX_SIZE}
#                max-file: ${DOCKER_LOG_MAX_FILE}
#        volumes_from:
#          - data
#        environment:
#            # Suppress all logging output other than error messages.
#            - LOGGING_QUIET=true
#            # Setting the default app for kibana to load, Twitter or Weibo
#            - KIBANA_DEFAULTAPPID="dashboard/Weibo"
#            # Whether to turn on the monitoring for ElasticSearch. Setting for ElasticSearch and Kibana should be the same.
#            # See https://www.elastic.co/guide/en/x-pack/5.0/monitoring-settings.html
#            - XPACK_MONITORING_ENABLED=false
#            - TZ
#            - DEBUG
#            # setting waiting seconds for check apps dependencies
#            - WAIT_SECS=900
#        # Set the hostname uniquely for each kibana instance
#        hostname: sfm_kibana_2
##        volumes:
##            - "../sfm-elk:/opt/sfm-elk"
#
#    logstash2:
#        image: gwul/sfm-logstash:master
#        links:
#            - mq:mq
#            - elasticsearch2:elasticsearch
#            - kibana2:kibana
#        ports:
#          - "5001:5000"
#        volumes_from:
#          - data
#        hostname: sfm_logstash_2
#        # To limit ELK loading to a specific collection set.
#        # command: --collection-set=1f0066fd9c464624aed612ddbf1fb407
#        environment:
#            - LS_JAVA_OPTS=-Xms2g -Xmx2g
#            - TZ
#            - DEBUG
#            - SFM_REQS=${ELK_REQS}
#            - RABBITMQ_USER
#            - RABBITMQ_PASSWORD
#            - SFM_UID
#            - SFM_GID
#            # setting waiting seconds for check apps dependencies
#            - WAIT_SECS=900
#            - SFM_UPGRADE_REQS=${UPGRADE_REQS}
##        volumes:
##            - "../sfm-elk:/opt/sfm-elk"
##            - "../sfm-twitter-harvester:/opt/sfm-twitter-harvester"
##            - "../sfm-weibo-harvester:/opt/sfm-weibo-harvester"
##            - "../sfm-utils:/opt/sfm-utils"
